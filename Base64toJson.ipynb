{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "from time import process_time\n",
    "from itertools import chain, starmap\n",
    "#starmap makes an iterator that computes the function using arguments obtained from the iterable.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import base64\n",
    "import sys\n",
    "import statistics\n",
    "\n",
    "\n",
    "#The function “flatten_iterative_solution” solved the nested data with an iterative approach. The idea \n",
    "#is that we scan each element in the data file and unpack just one level if the element is nested. \n",
    "#We keep iterating until all values are atomic elements (no dictionary or list).\n",
    "\n",
    "def flatten_iterative_solution(dictionary):\n",
    "    \"\"\"Flatten a nested data file\"\"\"\n",
    "\n",
    "    def unpack(parent_key, parent_value):\n",
    "        \"\"\"Unpack one level of nesting\"\"\"\n",
    "        \n",
    "        if isinstance(parent_value, dict):\n",
    "            for key, value in parent_value.items():\n",
    "                temp1 = parent_key + '_' + key\n",
    "                yield temp1, value\n",
    "        elif isinstance(parent_value, list):\n",
    "            i = 0 \n",
    "            for value in parent_value:\n",
    "                temp2 = parent_key + '_'+str(i) \n",
    "                i = i + 1\n",
    "                yield temp2, value\n",
    "        else:\n",
    "            yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "    # Keep iterating until the termination condition is satisfied\n",
    "    while True:\n",
    "        # Keep unpacking until all values are atomic elements (not dictionary or list)\n",
    "        dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "        # Terminate condition: not any value in dictionary or list\n",
    "        if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "           not any(isinstance(value, list) for value in dictionary.values()):\n",
    "                break\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def base64_to_json (base64array):\n",
    "    #decode base64 to json\n",
    "    reg_list = []   #  empty regular list\n",
    "    for row in range (len(base64array)): \n",
    "        temp = base64.b64decode(base64array[row], altchars=\"-_\")\n",
    "        reg_list.append(temp)\n",
    "        \n",
    "    return reg_list\n",
    "\n",
    "\n",
    "def median_in_Polygon(flat_dict): \n",
    "\n",
    "    for x in flat_dict:\n",
    "        length = len(str(flat_dict[x]))\n",
    "        \n",
    "        if (length >= 300):   #comments should be limited to 299 characters\n",
    "            pairs = flat_dict[x].replace(\"POLYGON\",\"\")  #The replace() method replaces a string with another string.\n",
    "            pairs = pairs.replace(\"(\",\"\")   # remove \"(\" and \")\" with replace() method\n",
    "            pairs = pairs.replace(\")\",\"\")\n",
    "            pair_list = [pair.split() for pair in pairs.split(',')]   #The split() method splits a string into a list\n",
    "            #with the help of transposed dataframe, to calculate median values. \n",
    "            df = pd.DataFrame([[float(pair[0]) for pair in pair_list],[float(pair[1]) for pair in pair_list]]).T\n",
    "            a = df[0].median()\n",
    "            b = df[1].median()\n",
    "            #print(a, \"br\", b)\n",
    "            flat_dict[x] = (a,b)\n",
    "        \n",
    "    return flat_dict\n",
    "\n",
    "def parse_JSON(reg_list):\n",
    "    #If you have a JSON string, you can parse it by using the json.loads() method, result will be a Python dictionary.\n",
    "    for x in reg_list:\n",
    "        d_dict = json.loads(x)\n",
    "\n",
    "    return d_dict\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "t1_start = process_time()  #start timer\n",
    "\n",
    "# Access data store\n",
    "data_store = pd.HDFStore('processed_10rows.h5')\n",
    "# Retrieve data using key\n",
    "preprocessed_df = data_store['10rows_df']\n",
    "data_store.close()\n",
    "\n",
    "#initializing\n",
    "start = 0\n",
    "end = start + 1\n",
    "rows = start + 10  #we have just 10 rows of data\n",
    "\n",
    "while (start < rows):\n",
    "    columnBase64 = preprocessed_df.loc[start: end,'B']  #decode column 'B', contains binary data, Base64\n",
    "    columnBase64_toArray = columnBase64.values   #df to numpy array\n",
    "    regular_list = base64_to_json(columnBase64_toArray)   \n",
    "    data_dict = parse_JSON(regular_list)\n",
    "    flatten_dict = flatten_iterative_solution(data_dict)  \n",
    "    result_dict = median_in_Polygon(flatten_dict)  \n",
    "    df = df.append(result_dict, ignore_index=True)\n",
    "    start = start + 1\n",
    "    end = start + 1\n",
    "\n",
    "# Stop the timer \n",
    "t1_stop = process_time() \n",
    "print(\"Elapsed time decode, 10 rows, in seconds:\", t1_stop-t1_start)  #typically 1 s\n",
    "print(df.shape)\n",
    "df   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
